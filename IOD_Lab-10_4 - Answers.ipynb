{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<div>\n","<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n","</div>"],"metadata":{"id":"3HgGfptrWJN0"}},{"cell_type":"markdown","source":["# Lab 10.4 - Analysing Streaming Data with Kafka"],"metadata":{"id":"5TdlZ1aoMido"}},{"cell_type":"markdown","source":["## Introduction"],"metadata":{"id":"jjiexW4Ic9C5"}},{"cell_type":"markdown","source":["**Note**: this notebook is to be run in Google Colab on your Google Drive. It will not work locally on your computer.\n","\n","The purpose of this lab is to gain experience in working with streaming data, using Apache Kafka installed on Google Colaboratory (Colab). You will see how a simple streaming dashboard data is created using the JupyterDash library and simulate live model scoring in monitoring a machine learning model.\n","\n","Apache Kafka is an open-source distributed publish-subscribe messaging system that maintains streams of messages in topics. It has become a highly popular streaming platform for real-time applications.\n","\n","In Google Colab, a virtual machine is automatically set up to execute your code. The maximum lifetime of such a machine is 12 hours. Note that notebooks will be disconnected from virtual machines if left idle. If this happens simple click on the Connect button to reconnect. If the kernel needs to be restarted (via the Runtime menu), variables may be lost but packages would not need to be reinstalled unless a new machine is assigned.\n","\n","https://research.google.com/colaboratory/faq.html\n","\n","Sign into colab.research.google.com and choose the Upload tab and upload this notebook.  This will automatically create a folder called \"Colab Notebooks\" in your Google Drive (if it does not already exist). Next upload the dataset **\"fraud_data.csv\"** into this \"Colab Notebooks\" folder by going firstly to drive.google.com."],"metadata":{"id":"H1TWbkec8Bhj"}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"QEQrAS-ZFvrg"}},{"cell_type":"markdown","source":["The following code connects your Google Drive to this notebook. A new window will open to prompt you to allow the connection to occur."],"metadata":{"id":"VDP3RABqIFKx"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"eO5HBG2nauQx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678696604264,"user_tz":-780,"elapsed":27935,"user":{"displayName":"Muru Raj","userId":"16263509272652930332"}},"outputId":"682d8f94-3bf2-4c02-c10e-e2baf6f972a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","source":["Upon running the following cell you should see the name of this notebook and fraud_data.csv."],"metadata":{"id":"fEsRfDUCNcLR"}},{"cell_type":"code","source":["!ls \"/content/gdrive/My Drive/Colab Notebooks/\""],"metadata":{"id":"CyHcYFw7a8HA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678696604265,"user_tz":-780,"elapsed":6,"user":{"displayName":"Muru Raj","userId":"16263509272652930332"}},"outputId":"4341883e-d155-4fad-af65-20ea13b10bb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vectors_and_Lin_alg_quiz.ipynb\n"]}]},{"cell_type":"markdown","source":["Next download and install Kafka:"],"metadata":{"id":"rbK5G8B3CYZl"}},{"cell_type":"code","source":["!curl -sSOL https://downloads.apache.org/kafka/3.4.0/kafka_2.13-3.4.0.tgz\n","!tar -xzf kafka_2.13-3.4.0.tgz"],"metadata":{"id":"qyZarLXNtSk1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678696605151,"user_tz":-780,"elapsed":889,"user":{"displayName":"Muru Raj","userId":"16263509272652930332"}},"outputId":"3f2250f8-3634-42bc-cc8e-bf0c13d5a48d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","gzip: stdin: not in gzip format\n","tar: Child returned status 1\n","tar: Error is not recoverable: exiting now\n"]}]},{"cell_type":"markdown","source":["The kafka-python library will provide a Python-like interface to the Kafka platform:"],"metadata":{"id":"TSQb5hZHDLny"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3_0eh5_tGgl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678696611346,"user_tz":-780,"elapsed":6199,"user":{"displayName":"Muru Raj","userId":"16263509272652930332"}},"outputId":"c8273b70-644c-48c3-aae6-cabcf2db048a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting kafka-python\n","  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: kafka-python\n","Successfully installed kafka-python-2.0.2\n"]}],"source":["!pip install kafka-python"]},{"cell_type":"markdown","source":["Finally jupyter-dash will be used for interactive plotting in this notebook:"],"metadata":{"id":"Kp9njQQ_DXAW"}},{"cell_type":"code","source":["!pip install jupyter-dash"],"metadata":{"id":"SDS0S6NFnQtc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678696619553,"user_tz":-780,"elapsed":8212,"user":{"displayName":"Muru Raj","userId":"16263509272652930332"}},"outputId":"9134c43c-b26e-4ff9-dd7f-8448c84f91c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting jupyter-dash\n","  Downloading jupyter_dash-0.4.2-py3-none-any.whl (23 kB)\n","Collecting ansi2html\n","  Downloading ansi2html-1.8.0-py3-none-any.whl (16 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from jupyter-dash) (2.25.1)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.9/dist-packages (from jupyter-dash) (5.3.4)\n","Collecting retrying\n","  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.9/dist-packages (from jupyter-dash) (7.9.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.9/dist-packages (from jupyter-dash) (2.2.3)\n","Collecting dash\n","  Downloading dash-2.8.1-py3-none-any.whl (9.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nest-asyncio\n","  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n","Collecting dash-core-components==2.0.0\n","  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n","Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from dash->jupyter-dash) (5.5.0)\n","Collecting dash-table==5.0.0\n","  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n","Collecting dash-html-components==2.0.0\n","  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n","Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.9/dist-packages (from flask->jupyter-dash) (2.2.3)\n","Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from flask->jupyter-dash) (3.1.2)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.9/dist-packages (from flask->jupyter-dash) (8.1.3)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/dist-packages (from flask->jupyter-dash) (2.1.2)\n","Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from flask->jupyter-dash) (6.0.0)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter-dash) (6.2)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter-dash) (6.1.12)\n","Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter-dash) (5.7.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython->jupyter-dash) (0.7.5)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython->jupyter-dash) (57.4.0)\n","Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython->jupyter-dash) (2.0.10)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython->jupyter-dash) (4.4.2)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython->jupyter-dash) (0.2.0)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython->jupyter-dash) (4.8.0)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython->jupyter-dash) (2.6.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->jupyter-dash) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->jupyter-dash) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->jupyter-dash) (1.26.14)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->jupyter-dash) (4.0.0)\n","Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from retrying->jupyter-dash) (1.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6.0->flask->jupyter-dash) (3.15.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython->jupyter-dash) (0.8.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>=3.0->flask->jupyter-dash) (2.1.2)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly>=5.0.0->dash->jupyter-dash) (8.2.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyter-dash) (0.2.6)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.9/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (23.2.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.9/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (2.8.2)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (5.2.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect->ipython->jupyter-dash) (0.7.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel->jupyter-dash) (3.1.0)\n","Installing collected packages: dash-table, dash-html-components, dash-core-components, retrying, nest-asyncio, jedi, ansi2html, dash, jupyter-dash\n","Successfully installed ansi2html-1.8.0 dash-2.8.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 jedi-0.18.2 jupyter-dash-0.4.2 nest-asyncio-1.5.6 retrying-1.3.4\n"]}]},{"cell_type":"code","source":["import time\n","import pandas as pd\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from kafka import KafkaProducer\n","from kafka import KafkaConsumer\n","\n","from jupyter_dash import JupyterDash\n","from dash import dcc\n","from dash import html\n","from dash.dependencies import (Input, Output)\n","import plotly.graph_objs as go\n","from plotly.subplots import make_subplots\n","\n","import re\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"aOmt5gH8tNd_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We use the following shell command to run Zookeeper and Kafka.\n","\n","Apache ZooKeeper is used in distributed systems for service synchronisation, tracking the status of nodes in the Kafka cluster and maintaining a list of Kafka topics and messages."],"metadata":{"id":"3S6gNu8jiEpe"}},{"cell_type":"code","source":["!./kafka_2.13-3.4.0/bin/zookeeper-server-start.sh -daemon ./kafka_2.13-3.4.0/config/zookeeper.properties\n","!./kafka_2.13-3.4.0/bin/kafka-server-start.sh -daemon ./kafka_2.13-3.4.0/config/server.properties\n","!echo \"Waiting until zookeeper and kafka services are ready\"\n","!sleep 10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ObSSLTHEtXpM","outputId":"d3a7c232-6936-4c49-bfe0-64627c5310df","executionInfo":{"status":"ok","timestamp":1678696633048,"user_tz":-780,"elapsed":9956,"user":{"displayName":"Muru Raj","userId":"16263509272652930332"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: ./kafka_2.13-3.2.0/bin/zookeeper-server-start.sh: No such file or directory\n","/bin/bash: ./kafka_2.13-3.2.0/bin/kafka-server-start.sh: No such file or directory\n","Waiting until zookeeper and kafka services are ready\n"]}]},{"cell_type":"markdown","source":["Verify that Kafka and Zookeeper are running (you should see four lines where the first two relate to Zookeeper and Kafka):"],"metadata":{"id":"rFWztBCBZvu1"}},{"cell_type":"code","source":["!ps -ef | grep kafka"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLRN2QIfuZ-N","executionInfo":{"status":"ok","timestamp":1678696633049,"user_tz":-780,"elapsed":16,"user":{"displayName":"Muru Raj","userId":"16263509272652930332"}},"outputId":"006c150c-ac5f-4d72-bcf1-a77edf52879a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root         824     256  0 08:37 ?        00:00:00 /bin/bash -c ps -ef | grep kafka\n","root         826     824  0 08:37 ?        00:00:00 grep kafka\n"]}]},{"cell_type":"markdown","source":["## Loading the dataset"],"metadata":{"id":"XtyI7IZWWKT4"}},{"cell_type":"markdown","source":["The dataset fraud_data.csv represents credit card transactions which have been labelled as fraudulent or non-fraudulent. It is synthetically generated and is based on the dataset at https://www.kaggle.com/kartik2112/fraud-detection. Some of the features based on transactions in the past 24 hours or 6 months have been engineered from the raw data.\n","\n","- trans_datetime - date and time of the transaction\n","- cc_num - credit card number of the customer\n","- merchant - name of the merchant to which the customer is paying\n","- amt: amount of the transaction in $\n","- first: first name of the customer\n","- last: last name of the customer\n","- gender: gender of the customer\n","- street, city, state: address of the customer\n","- zip: zip code of the transaction\n","- lat: latitude of the customer\n","- long: longitude of the customer\n","- city_pop: population of the city where the customer is living\n","- job, age: job and age of the customer\n","- num_trans_60d, num_trans_24h: number of transactions by this credit card in the past 60 days, 24 hours respectively\n","- num_fraud_trans_24h: number of fraudulent transactions by this credit card in the past 24 hours\n","- avg_trans_amt_60d: the average number of transactions by this credit card in the past 60 days\n","- is_fraud: whether the transaction is fraud or not (1 - fraud, 0 - not fraud)"],"metadata":{"id":"SXaev13DBX9n"}},{"cell_type":"code","source":["df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/fraud_data.csv')\n","df.head()"],"metadata":{"id":"A_GkHdyIyMmL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Perform EDA\n","\n","**Exercise**: Perform some exploratory data analysis on the df dataframe."],"metadata":{"id":"2AQ-zuTVbRh2"}},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"VjfgN_ig1DXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"1VV_3mf00de-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['category'].value_counts()"],"metadata":{"id":"b2_-U6ueEEBu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"U1apYwSY1NyV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Question**: How balanced is this dataset?"],"metadata":{"id":"SDCAhfQ-nfFL"}},{"cell_type":"code","source":["# ANSWER\n","df['is_fraud'].value_counts()"],"metadata":{"id":"afLHBhBt1kAx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ANSWER: 90% of data is non-fraud."],"metadata":{"id":"HLUfG9-wnfCq"}},{"cell_type":"markdown","source":["Correlation matrix:"],"metadata":{"id":"Ao0SNayvOpZN"}},{"cell_type":"code","source":["df_numerical = df[['amount', 'lat', 'lon', 'city_pop', 'age', 'num_trans_60d',\n","                   'num_trans_24h', 'num_fraud_trans_24h', 'avg_trans_amt_60d',\n","                   'is_fraud']]"],"metadata":{"id":"Qm9Tn-XmyJbQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["colormap = plt.cm.coolwarm\n","plt.figure(figsize = (10, 10))\n","plt.title('Pearson Correlation of Features', size = 15)\n","sns.heatmap(df_numerical.astype(float).corr(),\n","            linewidths = 0.1,\n","            vmax = 1.0,\n","            square = True,\n","            cmap = colormap,\n","            linecolor = 'white',\n","            annot = True)\n","plt.show()"],"metadata":{"id":"wlDWcWjObReN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We see that amount, num_trans_60d, num_fraud_trans_24h, and num_trans_avg_amt_60d have the strongest correlation with the target variable is_fraud."],"metadata":{"id":"9Utg_vlq0Kuu"}},{"cell_type":"markdown","source":["Since the dataset is large, we sample 1000 rows to obtain a pairplot."],"metadata":{"id":"6I5dc0r6Lwb8"}},{"cell_type":"code","source":["sns.pairplot(df_numerical.sample(1000, random_state=0))\n","plt.show()"],"metadata":{"id":"1xfPdJUQ1t79"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create topics\n","\n","Kafka records are stored in *topics* which can be thought of as data feeds that one can subscribe to.\n","\n","We shall create two topics on the Kafka platform.  \n","\n","- One with customer information (called *customerinfo*)\n","- One with predictors that can be used for a fraud prediction algorithm (called *features*)"],"metadata":{"id":"QUw-KHJ2k7GS"}},{"cell_type":"code","source":["!./kafka_2.13-3.4.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 2 --topic customerinfo\n","!./kafka_2.13-3.4.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic features\n"],"metadata":{"id":"gE5t92gmumtl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["View their details:"],"metadata":{"id":"InFNQJXcpN7d"}},{"cell_type":"code","source":["!./kafka_2.13-3.4.0/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic customerinfo\n","!./kafka_2.13-3.4.0/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic features"],"metadata":{"id":"kzNDHhEYuvjP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next we create the records for these topics. The records need to be a list of key-value pairs. Here we will make the key the datetime (timestamp) of the transaction."],"metadata":{"id":"nUsh8z9cxWvf"}},{"cell_type":"code","source":["timestamps = list(df['trans_datetime'][90000:].to_csv(index=False).split(\"\\n\"))\n","X_kafka = list(df.drop(['trans_datetime'],axis=1)[90000:].to_csv(index=False).split(\"\\n\"))\n"],"metadata":{"id":"9wCULKHpvpnB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictors = ['amount', 'num_trans_60d', 'num_fraud_trans_24h', 'avg_trans_amt_60d']\n","featuredata = list(df[predictors + ['is_fraud']][90000:].to_csv(index=False).split(\"\\n\"))"],"metadata":{"id":"3JQYujDjTC-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_kafka"],"metadata":{"id":"gKokg5sRRT_9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["featuredata"],"metadata":{"id":"4w_bJpc1x5J5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following helper functions will help write messages into each topic."],"metadata":{"id":"jZa-6vgBRT9q"}},{"cell_type":"code","source":["def error_callback(exc):\n","    raise Exception('Error while sending data to kafka: {0}'.format(str(exc)))\n","\n","def write_to_kafka(topic_name, items):\n","  count=0\n","  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n","  for key, message in items:\n","    producer.send(topic_name, key=key.encode('utf-8'),\n","                  value=message.encode('utf-8')).add_errback(error_callback)\n","    count+=1\n","  producer.flush()\n","  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))"],"metadata":{"id":"z07yz7k9v9oF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To start with we write 5 messages to each topic."],"metadata":{"id":"fitUBSoTs5vT"}},{"cell_type":"code","source":["write_to_kafka(\"customerinfo\", zip(timestamps[1:6], X_kafka[1:6]))\n","write_to_kafka(\"features\", zip(timestamps[1:6], featuredata[1:6]))"],"metadata":{"id":"ONUYkQDjwB1m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The zip function pairs up the timestamps with data into tuples."],"metadata":{"id":"_SBpKdontJHH"}},{"cell_type":"markdown","source":["Run the following shell command to view one of the topics. You will need to interrupt execution of the cell after the five messages have appeared."],"metadata":{"id":"YsyUexVDEOeK"}},{"cell_type":"code","source":["! /content/kafka_2.13-3.4.0/bin/kafka-console-consumer.sh \\\n","--bootstrap-server localhost:9092 \\\n","--topic features \\\n","--from-beginning"],"metadata":{"id":"_mtPpoL0wEDf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Subscribing to topics\n","To start with we subscribe to both topics with the same consumer."],"metadata":{"id":"ngb07baxSt8D"}},{"cell_type":"code","source":["kafka_bootstrap_servers = 'localhost:9092'\n","topics = ['customerinfo', 'features']"],"metadata":{"id":"qwIIXEBOopF6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["consumer = KafkaConsumer(\n","    *topics,\n","    bootstrap_servers=kafka_bootstrap_servers,\n","    auto_offset_reset='earliest',\n","    enable_auto_commit=True)"],"metadata":{"id":"vwy8yum2Og9H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["consumer.config"],"metadata":{"id":"PZ1EboF1F49_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following cell shows the message, key and value of records obtained by the consumer. Stop running of the cell once ten messages have been displayed."],"metadata":{"id":"1M2pZ1F7UDqK"}},{"cell_type":"code","source":["for message in consumer:\n","  print(\"message: \", message)\n","  print(\"key: \", message.key)\n","  print(\"value: \", message.value.decode('utf-8'))\n","  time.sleep(2)"],"metadata":{"id":"B1yrh4puEBfR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Question**: In which order have the records come into the consumer and are the records sorted within each topic?"],"metadata":{"id":"4dIBvs7-U5fD"}},{"cell_type":"markdown","source":["**Answer**: The records are sorted within each partition but not necessarily within a topic. One topic's results are shown followed by another."],"metadata":{"id":"cvsWSWGYVFdE"}},{"cell_type":"markdown","source":["For the next section we delete and recreate our topics, this time each with one partition."],"metadata":{"id":"b556EKw9_I9z"}},{"cell_type":"code","source":["!./kafka_2.13-3.4.0/bin/kafka-topics.sh --delete --bootstrap-server 127.0.0.1:9092 --topic customerinfo\n","!./kafka_2.13-3.4.0/bin/kafka-topics.sh --delete --bootstrap-server 127.0.0.1:9092 --topic features\n"],"metadata":{"id":"VeqKo6fLO9mk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!./kafka_2.13-3.4.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic customerinfo\n","!./kafka_2.13-3.4.0/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic features\n"],"metadata":{"id":"977NgNYVUStL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!./kafka_2.13-3.4.0/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic customerinfo\n","!./kafka_2.13-3.4.0/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic features"],"metadata":{"id":"2Lm3i6tuUStN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating a map showing the incoming stream"],"metadata":{"id":"ozyiWyE-PFP7"}},{"cell_type":"markdown","source":["This time we write more messages to each topic."],"metadata":{"id":"XmmvLZctPiVv"}},{"cell_type":"code","source":["write_to_kafka(\"customerinfo\", zip(timestamps[1:], X_kafka[1:]))"],"metadata":{"id":"rEH8J7WmUtsn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise**: write featuredata[1:] to the topic \"features\""],"metadata":{"id":"FrIVhkrOVX-E"}},{"cell_type":"code","source":["# ANSWER:\n","write_to_kafka(\"features\", zip(timestamps[1:], featuredata[1:]))"],"metadata":{"id":"_Bz2lT-T478e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["consumer = KafkaConsumer(\n","    'customerinfo',\n","    bootstrap_servers=kafka_bootstrap_servers,\n","    auto_offset_reset='earliest',\n","    enable_auto_commit=True)"],"metadata":{"id":"jRy9jZP94Ilx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run the following cell to see if the first 20 latitude and longitude values are accessible."],"metadata":{"id":"k1Vz9nAUQd0a"}},{"cell_type":"code","source":["df_received = pd.DataFrame(columns = ['lat', 'lon'])\n","count = 0\n","for message in consumer:\n","  count = count + 1\n","  x = re.split(r\",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\", message.value.decode('utf-8'))\n","  #source: https://stackoverflow.com/questions/18893390/splitting-on-comma-outside-quotes\n","  #this regular expression allows one to split the record only on commas occurring outside quote\n","  #characters\n","  df_received = df_received.append({'lat': x[11], 'lon': x[12]}, ignore_index=True)\n","  print(f\"lat = %s, lon = %s\" %  (x[11], x[12]))\n","  #time.sleep(1)\n","  if count > 20:\n","    break"],"metadata":{"id":"yy3lTBDH11rS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Another way of getting records from the consumer is through the poll method:"],"metadata":{"id":"pdMIYIg8RJvV"}},{"cell_type":"code","source":["latestrecord = consumer.poll(max_records = 1)"],"metadata":{"id":"DRwULg8bjSTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["latestrecord"],"metadata":{"id":"qeJC2YMDjTtf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["records = consumer.poll(max_records = 3)\n","for tp, consumer_records in records.items():\n","    for consumer_record in consumer_records:\n","        print(\"offset: \", consumer_record.offset, \"record value: \",\n","              consumer_record.value)"],"metadata":{"id":"myaF6qetIh8z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following function obtains the latest record from *consumer*."],"metadata":{"id":"r1angty4WBep"}},{"cell_type":"code","source":["def get_latest_record():\n","    global consumer\n","\n","    latestrecord = consumer.poll(max_records = 1)\n","    for tp, consumer_record in latestrecord.items():\n","      for message in consumer_record:\n","        x = re.split(r\",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\", message.value.decode('utf-8'))\n","\n","        y = message.key.decode('utf-8')\n","\n","    return [y] + x\n"],"metadata":{"id":"j0XB1TxlVuEB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise**: run get_latest_record() a couple of times to verify that different records are obtained from the consumer each time."],"metadata":{"id":"H14l0x-HWteS"}},{"cell_type":"code","source":["get_latest_record()"],"metadata":{"id":"yszoeo5C2hWs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Reinitialising the customerinfo *consumer*:"],"metadata":{"id":"shdheJrjXB8s"}},{"cell_type":"code","source":["consumer = KafkaConsumer(\n","    'customerinfo',\n","    bootstrap_servers=kafka_bootstrap_servers,\n","    auto_offset_reset='earliest',\n","    enable_auto_commit=True)"],"metadata":{"id":"AD85yfMf1qIz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following cell creates a dashboard app to display a map showing location of each customer Some additional data appears when hovering over a dot on the map."],"metadata":{"id":"ZHMEtm3RBR09"}},{"cell_type":"code","source":["try:\n","  del(trans_info)\n","except:\n","  pass\n","\n","trans_info = pd.DataFrame(columns = ['time', 'merchant', 'category', 'amount',\n","                                      'city', 'lat', 'lon', 'text'])\n","\n","app = JupyterDash(__name__)\n","\n","app.layout = html.Div([\n","    dcc.Graph(id='demo-live'),\n","    # every two seconds the layout updates:\n","    dcc.Interval(id='output-update', interval=2*1000)\n","])\n","\n","@app.callback(\n","    Output(component_id='demo-live', component_property='figure'),\n","    [Input(component_id='output-update', component_property='n_intervals')]\n",")\n","def get_live_updates(n_intervals):\n","    global trans_info\n","    newrow = get_latest_record() #newrow is a list\n","\n","    trans_info = trans_info.append({\n","        'time': newrow[0],\n","        'merchant': newrow[2],\n","        'category': newrow[3],\n","        'amount': newrow[4],\n","        'city': newrow[9],\n","        'lat': newrow[12],\n","        'lon': newrow[13],\n","        'text': 'Time: ' + newrow[0] + '<br>Merchant: ' + newrow[2] \\\n","        + '<br>Category: ' + newrow[3]\\\n","        + '<br>Amount: ' + newrow[4] + '' + '<br>City: '\\\n","        + newrow[9]\n","        }, ignore_index=True)\n","\n","    time.sleep(1)\n","    df2 = trans_info.copy()\n","\n","    data=go.Scattergeo(\n","          lon = df2['lon'],\n","          lat = df2['lat'],\n","          text = df2['text'],\n","          locationmode = 'USA-states',\n","          mode = 'markers',\n","        )\n","\n","    layout = go.Layout(\n","          autosize=False,\n","          width=780,\n","          height=500,\n","          margin=dict(l=20, r=20, t=20, b=20),\n","          paper_bgcolor='lightblue',\n","          geo_scope='usa',\n","          title_text='Locations of Transactions'\n","        )\n","    fig = {'data' : [data], 'layout' : layout}\n","\n","    return fig\n","\n"],"metadata":{"id":"W2Bz3AjbdeAV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Upon running the following code it may take a few moments for a map to appear. Hover your mouse over any of the dots to reveal more information. Aim to understand the workings of the previous cell."],"metadata":{"id":"LQCQOasE26-0"}},{"cell_type":"code","source":["app.run_server(mode='inline')"],"metadata":{"id":"4lNwG_4zTXNZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["While the app is running, run the following cell several times to verify that data is streaming into the consumer."],"metadata":{"id":"bK4ezf4KBDau"}},{"cell_type":"code","source":["trans_info.tail()"],"metadata":{"id":"COt462jYenL1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If there's a need to restart the consumer, one can run consumer.close() and then recreate the consumer."],"metadata":{"id":"Kgmhb6J_3uW2"}},{"cell_type":"markdown","source":["## Live predictions"],"metadata":{"id":"4XIjhzGztUAp"}},{"cell_type":"markdown","source":["Finally we use the features topic to make fraud predictions on the fly with a trained classification model.\n","\n","We use the first 90000 records of df as the training set."],"metadata":{"id":"RiIbJkaD62lz"}},{"cell_type":"code","source":["X_train = df[predictors][:90000]\n","y_train = df['is_fraud'][:90000]"],"metadata":{"id":"PbFePZ7-uV3E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise**: Fit a classification model to X_train, y_train and score it. (For this lab it does not matter how well or poorly the model performs.)"],"metadata":{"id":"M-qv94EHYjhk"}},{"cell_type":"code","source":["# ANSWER\n","from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(max_depth = 10, random_state = 0)"],"metadata":{"id":"87o8ayslvr3F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf.fit(X_train, y_train)"],"metadata":{"id":"hOkelL7TwC_g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf.score(df[predictors][90000:], df['is_fraud'][90000:])"],"metadata":{"id":"Akw6kwbKwaFS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Subscribe to the topic containing the predictors and perform live predictions."],"metadata":{"id":"X8mLXfbvxMRl"}},{"cell_type":"code","source":["feature_consumer = KafkaConsumer(\n","    'features',\n","    bootstrap_servers=kafka_bootstrap_servers,\n","    auto_offset_reset='earliest',\n","    enable_auto_commit=True)"],"metadata":{"id":"XhGZgQaByiAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_latest_customer_features():\n","    global feature_consumer\n","\n","    latestrecord = feature_consumer.poll(max_records = 1)\n","    for tp, consumer_record in latestrecord.items():\n","      for message in consumer_record:\n","        x = re.split(r\",\", message.value.decode('utf-8'))\n","        y = message.key.decode('utf-8')\n","\n","    return [y] + x"],"metadata":{"id":"wTfp__j_27Gc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_latest_customer_features() #may need to wait a few moments before the consumer is ready"],"metadata":{"id":"WwkqBSa73K80"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following is a function to be used in our next dashboard to output the type of predicted result (True/False Positives, True/False Negatives):"],"metadata":{"id":"Vxx0v8siZh0V"}},{"cell_type":"code","source":["def prediction_result(predicted_outcome, actual_outcome):\n","  # predicted   actual    output\n","  #    0           0        TN\n","  #    0           1        FN\n","  #    1           0        FP\n","  #    1           1        TP\n","\n","  key = predicted_outcome*2 + actual_outcome\n","  mappingdict = {0: 'TN', 1: 'FN', 2: 'FP', 3: 'TP'}\n","  return mappingdict[key]"],"metadata":{"id":"iBX6uMW_lz6c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["try:\n","  del(df_preds)\n","except:\n","  pass\n","df_preds = pd.DataFrame(columns = ['datetime'] + predictors +\n","                        ['predicted_output', 'actual_output', 'result'])"],"metadata":{"id":"nkdisgd65UmO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise**: The next cell creates a dashboard showing predictions made live. Fill in the missing code to generate predicted output based on your trained model."],"metadata":{"id":"VF3Z4MU7aFM-"}},{"cell_type":"code","source":["app = JupyterDash(__name__)\n","\n","app.layout = html.Div([\n","    dcc.Graph(id='demo-live'),\n","    ## for every 2 secs the layout updates\n","    dcc.Interval(id='output-update', interval=2*1000)\n","])\n","\n","@app.callback(\n","    Output(component_id='demo-live', component_property='figure'),\n","    [Input(component_id='output-update', component_property='n_intervals')]\n",")\n","def get_live_updates(n_intervals):\n","    global df_preds\n","    newrow = get_latest_customer_features()\n","    predicted_output = int(rf.predict([newrow[1:-1]]))\n","    df_preds = df_preds.append({'datetime': newrow[0],\n","                               'amount': newrow[1],\n","                               'num_trans_60d': newrow[2],\n","                               'num_fraud_trans_24h': newrow[3],\n","                               'avg_trans_amt_60d': newrow[4],\n","                               'predicted_output': predicted_output,\n","                               'actual_output': newrow[5],\n","                               'result': prediction_result(int(predicted_output),\n","                                                           int(newrow[5]))},\n","                               ignore_index=True)\n","\n","    time.sleep(1)\n","    df2 = df_preds.copy()\n","    last20 = df2.tail(20)\n","\n","    fig = make_subplots(\n","      rows=2, cols=2,\n","      row_heights=[0.1, 0.9],\n","      column_widths=[0.5, 0.5],\n","      specs=[[{\"type\": \"Table\", \"colspan\": 2}, None],\n","             [{\"type\": \"Histogram\"}, {\"type\": \"Table\"}]]\n","             )\n","\n","    fig.add_trace(\n","        go.Table(\n","            header=dict(\n","                values=[\"False Positive Count\", \"False Negative Count\"],\n","                font=dict(size=14),\n","                align=\"center\"\n","            ),\n","            cells=dict(\n","                values=[sum(df_preds['result'] == 'FP'),\n","                        sum(df_preds['result'] == 'FN')],\n","            align = \"center\"),\n","            columnwidth = [.5, .5]\n","        ),\n","        row=1, col=1\n","    )\n","\n","    fig.add_trace(\n","      go.Histogram(x=df2['amount'].astype(float),\n","                 xbins = dict(start = 0, end=500, size=20)\n","                 ),\n","      row=2, col=1\n","    )\n","    fig.update_xaxes(title_text=\"Amount Spent\", row=2, col=1)\n","    fig.update_yaxes(title_text=\"Count\", row=2, col=1)\n","\n","    fig.add_trace(\n","      go.Table(\n","        header=dict(\n","            values=[\"Datetime\", \"Predicted Outcome\", \"Actual Outcome\",\n","                    \"Result\"],\n","            font=dict(size=14),\n","            align=\"center\"\n","        ),\n","        cells=dict(\n","            values=[last20['datetime'], last20['predicted_output'],\n","                              last20['actual_output'], last20['result']],\n","            align = \"center\"),\n","        columnwidth = [.27, .27, .27, .19]\n","      ),\n","    row=2, col=2\n","    )\n","    fig.update_layout(\n","    height=800,\n","    showlegend=False,\n","    title_text=\"Live Results\",\n","    )\n","    return fig"],"metadata":{"id":"XunkNKYdHbpB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["app.run_server(mode='inline')"],"metadata":{"id":"sl68stEkA1JG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise**: While the app is running verify that df_preds is being modified."],"metadata":{"id":"VFf2wf39Rkjm"}},{"cell_type":"code","source":["df_preds.tail()"],"metadata":{"id":"RlQn6TALa0SC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Conclusion\n","We have implemented a simple Kakfa streaming platform to which we published to topics - one with customer information, the other a subset of that to make fraud predictions. With the first topic we generated a live map showing the locations of customers, in the other we created a live predictive model scoring dashboard."],"metadata":{"id":"uayokLMZa8-A"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","> > > > > > > > > © 2023 Institute of Data\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n"],"metadata":{"id":"W32tNIdCIFvO"}}]}